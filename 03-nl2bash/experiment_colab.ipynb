{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZWC_MgV0tFV"
      },
      "source": [
        "# NL2Bash Depth Experiment — Colab Notebook\n",
        "\n",
        "This notebook runs depth experiments comparing PyTorch and TorchLite models on the NL2Bash dataset.\n",
        "\n",
        "**Features:**\n",
        "- Train transformer models at various depths\n",
        "- Plot loss curves and final metrics\n",
        "- Compute exact match accuracy\n",
        "- Save results to Google Drive\n",
        "\n",
        "**Run time estimates:**\n",
        "- PyTorch (all depths): ~30-60 min on GPU\n",
        "- TorchLite (depth=1): ~5 min on CPU\n",
        "- Both experiments: ~1 hour total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H5_2Oq40tFW"
      },
      "source": [
        "## 1. Setup & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kD_8R8h80tFW",
        "outputId": "74d1be92-7937-431d-aea1-cbeddab1eb28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Dependencies installed\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q matplotlib numpy\n",
        "\n",
        "print(\"✓ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7_C_K4Kz0tFX",
        "outputId": "462e5df0-dde2-4d24-fbff-7454744b5533",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✓ Google Drive mounted\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive (optional but recommended for saving results)\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "print(\"✓ Google Drive mounted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SX7cX2gP0tFX",
        "outputId": "0d5fdb76-fa3d-481b-e389-f5c43248aaa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# github.com:22 SSH-2.0-fa646f3\n",
            "# github.com:22 SSH-2.0-fa646f3\n",
            "# github.com:22 SSH-2.0-fa646f3\n",
            "# github.com:22 SSH-2.0-fa646f3\n",
            "# github.com:22 SSH-2.0-fa646f3\n",
            "fatal: destination path 'ground-up-vla' already exists and is not an empty directory.\n",
            "✓ Working directory: /content\n"
          ]
        }
      ],
      "source": [
        "# Clone or download the nl2bash project\n",
        "import os\n",
        "os.chdir('/content')\n",
        "\n",
        "# Option 1: Clone from git (if available)\n",
        "!mkdir -p ~/.ssh\n",
        "!ssh-keyscan github.com >> ~/.ssh/known_hosts\n",
        "!git clone git@github.com:ng3gn/ground-up-vla.git\n",
        "\n",
        "# Option 2: Upload files manually or use Drive\n",
        "# For now, we'll create the necessary structure\n",
        "\n",
        "PROJECT_DIR = '/content/03-nl2bash'\n",
        "#os.makedirs(PROJECT_DIR, exist_ok=True)\n",
        "#os.chdir(PROJECT_DIR)\n",
        "\n",
        "print(f\"✓ Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nVJH_cK0tFX"
      },
      "source": [
        "## 2. Copy Project Files from Drive or Upload\n",
        "\n",
        "**Choose one approach:**\n",
        "- **Option A:** Copy from Google Drive (if you've uploaded the project)\n",
        "- **Option B:** Upload ZIP file and extract\n",
        "- **Option C:** Use the cell below to set up minimal files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3vLDdXRl0tFY",
        "outputId": "713bf48b-7a43-4144-fe48-f18e9d941944",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/03.zip\n",
            "   creating: 03-nl2bash/\n",
            "   creating: 03-nl2bash/data/\n",
            "  inflating: 03-nl2bash/data/all.nl  \n",
            "  inflating: 03-nl2bash/data/all.cm  \n",
            "  inflating: 03-nl2bash/data/shared_vocab.txt  \n",
            "  inflating: 03-nl2bash/data/README.md  \n",
            "  inflating: 03-nl2bash/data/cm_vocab.txt  \n",
            "  inflating: 03-nl2bash/data/nl_vocab.txt  \n",
            "  inflating: 03-nl2bash/data/LICENSE  \n",
            "   creating: 03-nl2bash/model/\n",
            "   creating: 03-nl2bash/model/__pycache__/\n",
            "  inflating: 03-nl2bash/model/__pycache__/full.cpython-311.pyc  \n",
            "  inflating: 03-nl2bash/model/__pycache__/transformer_lite.cpython-314.pyc  \n",
            "  inflating: 03-nl2bash/model/__pycache__/__init__.cpython-314.pyc  \n",
            "  inflating: 03-nl2bash/model/__pycache__/__init__.cpython-311.pyc  \n",
            "  inflating: 03-nl2bash/model/__pycache__/transformer_full.cpython-314.pyc  \n",
            "  inflating: 03-nl2bash/model/__pycache__/lite.cpython-311.pyc  \n",
            "  inflating: 03-nl2bash/model/__pycache__/full.cpython-314.pyc  \n",
            "  inflating: 03-nl2bash/model/__pycache__/lite.cpython-314.pyc  \n",
            "  inflating: 03-nl2bash/model/lite.py  \n",
            "  inflating: 03-nl2bash/model/full.py  \n",
            "  inflating: 03-nl2bash/model/transformer_lite.py  \n",
            "  inflating: 03-nl2bash/model/transformer_full.py  \n",
            "  inflating: 03-nl2bash/model/__init__.py  \n",
            "  inflating: 03-nl2bash/main.py      \n",
            "   creating: 03-nl2bash/.claude/\n",
            "  inflating: 03-nl2bash/.claude/settings.local.json  \n",
            "  inflating: 03-nl2bash/tokenizer.py  \n",
            "  inflating: 03-nl2bash/README.md    \n",
            "  inflating: 03-nl2bash/experiment_colab.ipynb  \n",
            "  inflating: 03-nl2bash/dataset.py   \n",
            "  inflating: 03-nl2bash/PLAN.md      \n",
            "  inflating: 03-nl2bash/__init__.py  \n",
            "  inflating: 03-nl2bash/vocab.py     \n",
            "  inflating: 03-nl2bash/SUMMARY.md   \n",
            "  inflating: 03-nl2bash/PYTORCH_VS_TORCHLITE.md  \n",
            "  inflating: 03-nl2bash/experiment.py  \n",
            "Files should be in: /content/03-nl2bash\n",
            "Required files:\n",
            "  - vocab.py\n",
            "  - tokenizer.py\n",
            "  - dataset.py\n",
            "  - main.py\n",
            "  - model/transformer_full.py\n",
            "  - model/transformer_lite.py\n",
            "  - data/ (directory with vocab and data files)\n"
          ]
        }
      ],
      "source": [
        "# OPTION A: Copy from Google Drive\n",
        "# Adjust the path to where your nl2bash project is stored\n",
        "# import shutil\n",
        "# shutil.copytree('/content/drive/MyDrive/path/to/nl2bash', '/content/nl2bash', dirs_exist_ok=True)\n",
        "\n",
        "# OPTION B: Upload from local machine\n",
        "# Use the file upload widget in Colab's file browser\n",
        "\n",
        "!unzip /content/03.zip\n",
        "!unzip /content/torchlite.zip\n",
        "\n",
        "\n",
        "# OPTION C: We'll assume files are already in place\n",
        "print(\"Files should be in:\", PROJECT_DIR)\n",
        "print(\"Required files:\")\n",
        "print(\"  - vocab.py\")\n",
        "print(\"  - tokenizer.py\")\n",
        "print(\"  - dataset.py\")\n",
        "print(\"  - main.py\")\n",
        "print(\"  - model/transformer_full.py\")\n",
        "print(\"  - model/transformer_lite.py\")\n",
        "print(\"  - data/ (directory with vocab and data files)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QX3j7yAj0tFY"
      },
      "source": [
        "## 3. Verify Setup\n",
        "\n",
        "Check that all required files are present."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9f_8XT1D0tFY",
        "outputId": "c30f14fa-62a7-41e6-d896-6d24982fe6dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ vocab.py\n",
            "✓ tokenizer.py\n",
            "✓ dataset.py\n",
            "✓ main.py\n",
            "✓ model/transformer_full.py\n",
            "✓ model/transformer_lite.py\n",
            "\n",
            "✓ All required files present!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "required_files = [\n",
        "    'vocab.py',\n",
        "    'tokenizer.py',\n",
        "    'dataset.py',\n",
        "    'main.py',\n",
        "    'model/transformer_full.py',\n",
        "    'model/transformer_lite.py',\n",
        "]\n",
        "\n",
        "missing = []\n",
        "for f in required_files:\n",
        "    path = os.path.join(PROJECT_DIR, f)\n",
        "    if os.path.exists(path):\n",
        "        print(f\"✓ {f}\")\n",
        "    else:\n",
        "        print(f\"✗ {f}\")\n",
        "        missing.append(f)\n",
        "\n",
        "if missing:\n",
        "    print(f\"\\n⚠️  Missing files: {missing}\")\n",
        "    print(\"Please upload or copy the project files first.\")\n",
        "else:\n",
        "    print(\"\\n✓ All required files present!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Yz-7a3c30tFY",
        "outputId": "efc374ca-d89f-4a20-b0c6-3c6061ee0bb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data files:\n",
            "✓ shared_vocab.txt (0.1 MB)\n",
            "✓ all.nl (1.0 MB)\n",
            "✓ all.cm (0.5 MB)\n",
            "\n",
            "✓ All data files present!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "data_dir = os.path.join(PROJECT_DIR, 'data')\n",
        "required_data = [\n",
        "    'shared_vocab.txt',\n",
        "    'all.nl',\n",
        "    'all.cm',\n",
        "]\n",
        "\n",
        "print(\"Data files:\")\n",
        "missing_data = []\n",
        "for f in required_data:\n",
        "    path = os.path.join(data_dir, f)\n",
        "    if os.path.exists(path):\n",
        "        size = os.path.getsize(path) / 1024 / 1024\n",
        "        print(f\"✓ {f} ({size:.1f} MB)\")\n",
        "    else:\n",
        "        print(f\"✗ {f}\")\n",
        "        missing_data.append(f)\n",
        "\n",
        "if missing_data:\n",
        "    print(f\"\\n⚠️  Missing data files: {missing_data}\")\n",
        "    print(\"Please upload the data directory.\")\n",
        "else:\n",
        "    print(\"\\n✓ All data files present!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yy3SQK_0tFZ"
      },
      "source": [
        "## 4. Import Modules & Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jKkPFT9x0tFZ",
        "outputId": "72d9bf17-5d5d-41a6-c78a-1d457a8d3404",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ All modules imported successfully\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Add project directory to path\n",
        "sys.path.insert(0, PROJECT_DIR)\n",
        "sys.path.insert(0, os.path.dirname(PROJECT_DIR))  # For torchlite imports\n",
        "\n",
        "from vocab import Vocabulary\n",
        "from tokenizer import NL2BashTokenizer\n",
        "from dataset import NL2BashDataset, create_pytorch_dataloader, create_torchlite_dataloader\n",
        "\n",
        "print(\"✓ All modules imported successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "T6C0w-sZ0tFZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cbd753d-3436-494c-df41-bc0a4d9d6caa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Configuration loaded\n",
            "  PyTorch depths: [1, 2, 4, 8, 16]\n",
            "  TorchLite depths: [1]\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "PYTORCH_CONFIG = {\n",
        "    'd_model': [128, 256, 512, 1024],\n",
        "    'n_heads': 1,\n",
        "    'd_ff': [4*128, 4*256, 4*512, 4*1024],\n",
        "    'batch_size': 16,\n",
        "    'lr': 0.0001,\n",
        "    'n_epochs': 20,\n",
        "    'depths': [1, 2, 4, 8, 16],\n",
        "    'dropout': 0.0,\n",
        "    'max_len': 128,\n",
        "}\n",
        "\n",
        "TORCHLITE_CONFIG = {\n",
        "    'd_model': 128,\n",
        "    'n_heads': 1,\n",
        "    'd_ff': 512,\n",
        "    'batch_size': 16,\n",
        "    'lr': 0.001,\n",
        "    'n_epochs': 3,\n",
        "    'depths': [1],\n",
        "    'max_len': 128,\n",
        "}\n",
        "\n",
        "# Output directory (use Drive for persistence)\n",
        "OUTPUT_DIR = os.path.join(PROJECT_DIR, 'experiment_outputs')\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"✓ Configuration loaded\")\n",
        "print(f\"  PyTorch depths: {PYTORCH_CONFIG['depths']}\")\n",
        "print(f\"  TorchLite depths: {TORCHLITE_CONFIG['depths']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3S_Yf5_0tFZ"
      },
      "source": [
        "## 5. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "I9pEQ-jd0tFa",
        "outputId": "144602bc-8d43-4d21-8fde-7672d760d0a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Loading dataset from /content/03-nl2bash/data/all.nl and /content/03-nl2bash/data/all.cm...\n",
            "  Loaded 12607 examples\n",
            "\n",
            "Dataset split:\n",
            "  Train: 10505 examples\n",
            "  Dev:   1050 examples\n",
            "  Test:  1052 examples\n",
            "✓ Data loaded\n",
            "  Vocab size: 9078\n",
            "  Train examples: 10505\n",
            "  Dev examples: 1050\n",
            "  Test examples: 1052\n"
          ]
        }
      ],
      "source": [
        "def load_data(data_dir='data'):\n",
        "    \"\"\"Load vocabulary, tokenizer, and dataset.\"\"\"\n",
        "    data_dir = os.path.join(PROJECT_DIR, data_dir)\n",
        "    vocab_path = os.path.join(data_dir, 'shared_vocab.txt')\n",
        "    vocab = Vocabulary.load(vocab_path)\n",
        "    tokenizer = NL2BashTokenizer(vocab)\n",
        "\n",
        "    nl_file = os.path.join(data_dir, 'all.nl')\n",
        "    cm_file = os.path.join(data_dir, 'all.cm')\n",
        "    dataset = NL2BashDataset(nl_file, cm_file, tokenizer)\n",
        "\n",
        "    train_dataset, dev_dataset, test_dataset = dataset.split(\n",
        "        train_ratio=10, dev_ratio=1, test_ratio=1, seed=42\n",
        "    )\n",
        "    return vocab, tokenizer, train_dataset, dev_dataset, test_dataset\n",
        "\n",
        "print(\"Loading data...\")\n",
        "vocab, tokenizer, train_dataset, dev_dataset, test_dataset = load_data()\n",
        "\n",
        "print(f\"✓ Data loaded\")\n",
        "print(f\"  Vocab size: {len(vocab)}\")\n",
        "print(f\"  Train examples: {len(train_dataset)}\")\n",
        "print(f\"  Dev examples: {len(dev_dataset)}\")\n",
        "print(f\"  Test examples: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_zzUUz40tFa"
      },
      "source": [
        "## 6. Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7VCJp23F0tFa",
        "outputId": "bb3a0eef-1f66-46e9-ee5f-1f199e003ee3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ PyTorch helper functions defined\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def compute_exact_match_pytorch(model, dataset, tokenizer, device, num_samples=100):\n",
        "    \"\"\"Generate commands and compute exact match accuracy (PyTorch).\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = min(num_samples, len(dataset))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(total):\n",
        "            ex = dataset[i]\n",
        "            nl_length = ex['nl_length']\n",
        "            nl_ids = ex['combined_ids'][:nl_length]\n",
        "            nl_tokens = torch.LongTensor([nl_ids]).to(device)\n",
        "\n",
        "            generated = model.generate(\n",
        "                nl_tokens,\n",
        "                start_id=tokenizer.vocab.start_id,\n",
        "                end_id=tokenizer.vocab.end_id,\n",
        "                max_len=64\n",
        "            )\n",
        "\n",
        "            generated_ids = generated[0].cpu().tolist()\n",
        "            generated_text = tokenizer.decode_cm(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "            if generated_text.strip() == ex['cm_text'].strip():\n",
        "                correct += 1\n",
        "\n",
        "    return correct / total if total > 0 else 0.0\n",
        "\n",
        "print(\"✓ PyTorch helper functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "2f_oHPPb0tFa",
        "outputId": "5de3defa-024f-481d-f9a4-f1151861fc73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ TorchLite helper functions defined\n"
          ]
        }
      ],
      "source": [
        "def compute_exact_match_lite(model, dataset, tokenizer, num_samples=20):\n",
        "    \"\"\"Generate commands and compute exact match accuracy (TorchLite).\"\"\"\n",
        "    correct = 0\n",
        "    total = min(num_samples, len(dataset))\n",
        "\n",
        "    for i in range(total):\n",
        "        ex = dataset[i]\n",
        "        nl_length = ex['nl_length']\n",
        "        nl_ids = ex['combined_ids'][:nl_length]\n",
        "        nl_tokens = np.array([nl_ids], dtype=np.int64)\n",
        "\n",
        "        generated = model.generate(\n",
        "            nl_tokens,\n",
        "            start_id=tokenizer.vocab.start_id,\n",
        "            end_id=tokenizer.vocab.end_id,\n",
        "            max_len=64\n",
        "        )\n",
        "\n",
        "        generated_ids = generated[0].tolist()\n",
        "        generated_text = tokenizer.decode_cm(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "        if generated_text.strip() == ex['cm_text'].strip():\n",
        "            correct += 1\n",
        "\n",
        "    return correct / total if total > 0 else 0.0\n",
        "\n",
        "print(\"✓ TorchLite helper functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MlX3jvVs0tFa",
        "outputId": "057877de-99a8-4b74-d23f-a1b2f7402949",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Plotting functions defined\n"
          ]
        }
      ],
      "source": [
        "def plot_depth_comparison(results, framework_name, config, output_path):\n",
        "    \"\"\"Plot train and dev loss curves for each depth.\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    fig.suptitle(f'{framework_name} — Depth Comparison (d_model={config[\"d_model\"]})', fontsize=14)\n",
        "\n",
        "    colors = plt.cm.viridis(np.linspace(0, 0.9, len(results)))\n",
        "\n",
        "    for (depth, data), color in zip(sorted(results.items()), colors):\n",
        "        epochs = list(range(1, len(data['train_losses']) + 1))\n",
        "        label = f'n_layers={depth} ({data[\"n_params\"]:,} params)'\n",
        "\n",
        "        ax1.plot(epochs, data['train_losses'], color=color, label=label, marker='o', markersize=3)\n",
        "        ax2.plot(epochs, data['dev_losses'], color=color, label=label, marker='o', markersize=3)\n",
        "\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Training Loss')\n",
        "    ax1.legend(fontsize=8)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.set_title('Dev Loss')\n",
        "    ax2.legend(fontsize=8)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"Saved plot to {output_path}\")\n",
        "\n",
        "print(\"✓ Plotting functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0mM_Mv8F0tFb",
        "outputId": "975ec550-82b6-4ebb-c51d-7a5cd8351365",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Bar chart functions defined\n"
          ]
        }
      ],
      "source": [
        "def plot_final_bar_chart(results, framework_name, config, output_path):\n",
        "    \"\"\"Bar chart of final train/dev/test loss and exact match per depth.\"\"\"\n",
        "    depths = sorted(results.keys())\n",
        "    train_vals = [results[d]['final_train_loss'] for d in depths]\n",
        "    dev_vals = [results[d]['final_dev_loss'] for d in depths]\n",
        "    test_vals = [results[d]['final_test_loss'] for d in depths]\n",
        "    em_vals = [results[d]['exact_match'] for d in depths]\n",
        "\n",
        "    x = np.arange(len(depths))\n",
        "    width = 0.22\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    fig.suptitle(f'{framework_name} — Final Metrics by Depth (d_model={config[\"d_model\"]})', fontsize=14)\n",
        "\n",
        "    # Loss bar chart\n",
        "    ax1.bar(x - width, train_vals, width, label='Train', color='#2196F3')\n",
        "    ax1.bar(x, dev_vals, width, label='Dev', color='#FF9800')\n",
        "    ax1.bar(x + width, test_vals, width, label='Test', color='#4CAF50')\n",
        "    ax1.set_xlabel('n_layers')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Final Loss')\n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels([str(d) for d in depths])\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Exact match bar chart\n",
        "    bars = ax2.bar(x, [v * 100 for v in em_vals], width * 2, color='#9C27B0')\n",
        "    ax2.set_xlabel('n_layers')\n",
        "    ax2.set_ylabel('Exact Match (%)')\n",
        "    ax2.set_title('Test Exact Match Accuracy')\n",
        "    ax2.set_xticks(x)\n",
        "    ax2.set_xticklabels([str(d) for d in depths])\n",
        "    ax2.grid(True, alpha=0.3, axis='y')\n",
        "    # Add value labels on bars\n",
        "    for bar, val in zip(bars, em_vals):\n",
        "        ax2.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5,\n",
        "                 f'{val:.1%}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"Saved bar chart to {output_path}\")\n",
        "\n",
        "print(\"✓ Bar chart functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ktEMQFzB0tFb",
        "outputId": "0fbb9cb1-e4af-4071-cc62-e7be747d888a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Results saving functions defined\n"
          ]
        }
      ],
      "source": [
        "def save_results(results, output_path):\n",
        "    \"\"\"Save results dict to JSON.\"\"\"\n",
        "    serializable = {}\n",
        "    for depth, data in results.items():\n",
        "        serializable[str(depth)] = {\n",
        "            'train_losses': [float(x) for x in data['train_losses']],\n",
        "            'dev_losses': [float(x) for x in data['dev_losses']],\n",
        "            'final_train_loss': float(data['final_train_loss']),\n",
        "            'final_dev_loss': float(data['final_dev_loss']),\n",
        "            'final_test_loss': float(data['final_test_loss']),\n",
        "            'exact_match': float(data['exact_match']),\n",
        "            'n_params': int(data['n_params']),\n",
        "        }\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(serializable, f, indent=2)\n",
        "    print(f\"Saved results to {output_path}\")\n",
        "\n",
        "print(\"✓ Results saving functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z_yqIVT0tFb"
      },
      "source": [
        "## 7. Run PyTorch Experiment\n",
        "\n",
        "⚠️ This takes 30-60 minutes with GPU acceleration. You can reduce the `depths` list or `n_epochs` for faster runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "p0lXXjBb0tFb",
        "outputId": "064a72e2-285f-4d4a-9ec8-54f210532efe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ready to run PyTorch experiment\n",
            "Depths: [1, 2, 4, 8, 16]\n",
            "Epochs: 20\n",
            "Batch size: 16\n"
          ]
        }
      ],
      "source": [
        "# Optional: Reduce config for faster testing\n",
        "# PYTORCH_CONFIG['depths'] = [1]  # Test only depth 1\n",
        "# PYTORCH_CONFIG['n_epochs'] = 2  # Test only 2 epochs\n",
        "\n",
        "print(\"Ready to run PyTorch experiment\")\n",
        "print(f\"Depths: {PYTORCH_CONFIG['depths']}\")\n",
        "print(f\"Epochs: {PYTORCH_CONFIG['n_epochs']}\")\n",
        "print(f\"Batch size: {PYTORCH_CONFIG['batch_size']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "4dL0q_F90tFb",
        "outputId": "c93b556d-731f-4276-e19c-63a73509ae6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  torchlite.zip\n",
            "replace torchlite/__pycache__/logger.cpython-314.pyc? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: torchlite/__pycache__/logger.cpython-314.pyc  \n",
            "replace torchlite/__pycache__/visualize.cpython-314.pyc? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: torchlite/__pycache__/visualize.cpython-314.pyc  \n",
            "  inflating: torchlite/__pycache__/optim.cpython-311.pyc  \n",
            "  inflating: torchlite/__pycache__/nn.cpython-311.pyc  \n",
            "  inflating: torchlite/__pycache__/nn.cpython-314.pyc  \n",
            "  inflating: torchlite/__pycache__/tensor.cpython-311.pyc  \n",
            "  inflating: torchlite/__pycache__/visualize.cpython-311.pyc  \n",
            "  inflating: torchlite/__pycache__/__init__.cpython-314.pyc  \n",
            "  inflating: torchlite/__pycache__/optim.cpython-314.pyc  \n",
            "  inflating: torchlite/__pycache__/__init__.cpython-311.pyc  \n",
            "  inflating: torchlite/__pycache__/logger.cpython-311.pyc  \n",
            "  inflating: torchlite/__pycache__/tensor.cpython-314.pyc  \n",
            "  inflating: torchlite/logger.py     \n",
            "  inflating: torchlite/visualize.py  \n",
            "  inflating: torchlite/README.md     \n",
            "  inflating: torchlite/nn.py         \n",
            "  inflating: torchlite/__init__.py   \n",
            "  inflating: torchlite/optim.py      \n",
            "  inflating: torchlite/tensor.py     \n",
            "✓ PyTorch experiment function defined\n"
          ]
        }
      ],
      "source": [
        "# Import PyTorch training functions\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/03-nl2bash')\n",
        "\n",
        "!unzip torchlite.zip\n",
        "\n",
        "from model.transformer_full import TransformerDecoder\n",
        "\n",
        "from main import compute_masked_loss, train_epoch, evaluate\n",
        "\n",
        "def run_pytorch_experiment(vocab, tokenizer, train_dataset, dev_dataset, test_dataset, config):\n",
        "    \"\"\"Train PyTorch models at each depth and return loss histories + final metrics.\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"\\nPyTorch device: {device}\")\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for depth in config['depths']:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"PyTorch: Training depth={depth}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        model = TransformerDecoder(\n",
        "            vocab_size=len(vocab),\n",
        "            d_model=config['d_model'],\n",
        "            n_heads=config['n_heads'],\n",
        "            n_layers=depth,\n",
        "            d_ff=config['d_ff'],\n",
        "            dropout=config['dropout'],\n",
        "            max_len=config['max_len'],\n",
        "        ).to(device)\n",
        "\n",
        "        n_params = sum(p.numel() for p in model.parameters())\n",
        "        print(f\"  Parameters: {n_params:,}\")\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
        "\n",
        "        train_loader = create_pytorch_dataloader(\n",
        "            train_dataset, config['batch_size'], shuffle=True, pad_id=vocab.pad_id\n",
        "        )\n",
        "        dev_loader = create_pytorch_dataloader(\n",
        "            dev_dataset, config['batch_size'], shuffle=False, pad_id=vocab.pad_id\n",
        "        )\n",
        "        test_loader = create_pytorch_dataloader(\n",
        "            test_dataset, config['batch_size'], shuffle=False, pad_id=vocab.pad_id\n",
        "        )\n",
        "\n",
        "        train_losses = []\n",
        "        dev_losses = []\n",
        "\n",
        "        for epoch in range(1, config['n_epochs'] + 1):\n",
        "            train_metrics = train_epoch(model, train_loader, optimizer, device, epoch, log_interval=9999)\n",
        "            dev_metrics = evaluate(model, dev_loader, device)\n",
        "\n",
        "            train_losses.append(train_metrics['loss'])\n",
        "            dev_losses.append(dev_metrics['loss'])\n",
        "\n",
        "            print(f\"  Epoch {epoch:2d}/{config['n_epochs']} | \"\n",
        "                  f\"Train loss: {train_metrics['loss']:.4f} | \"\n",
        "                  f\"Dev loss: {dev_metrics['loss']:.4f} | \"\n",
        "                  f\"Time: {train_metrics['time']:.1f}s\")\n",
        "\n",
        "        # Final evaluation on test set\n",
        "        test_metrics = evaluate(model, test_loader, device)\n",
        "        print(f\"\\n  Test loss: {test_metrics['loss']:.4f}\")\n",
        "\n",
        "        # Exact match accuracy on test set\n",
        "        print(f\"  Computing exact match on test set...\")\n",
        "        exact_match = compute_exact_match_pytorch(model, test_dataset, tokenizer, device, num_samples=100)\n",
        "        print(f\"  Exact match accuracy: {exact_match:.2%}\")\n",
        "\n",
        "        results[depth] = {\n",
        "            'train_losses': train_losses,\n",
        "            'dev_losses': dev_losses,\n",
        "            'final_train_loss': train_losses[-1],\n",
        "            'final_dev_loss': dev_losses[-1],\n",
        "            'final_test_loss': test_metrics['loss'],\n",
        "            'exact_match': exact_match,\n",
        "            'n_params': n_params,\n",
        "        }\n",
        "\n",
        "        print(f\"\\n  >> depth={depth}: \"\n",
        "              f\"train={train_losses[-1]:.4f} | \"\n",
        "              f\"dev={dev_losses[-1]:.4f} | \"\n",
        "              f\"test={test_metrics['loss']:.4f} | \"\n",
        "              f\"exact_match={exact_match:.1%}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"✓ PyTorch experiment function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "7cdN_5W60tFc",
        "outputId": "59c4f73a-db11-423f-8964-e19e9daaa8b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "PYTORCH EXPERIMENT\n",
            "============================================================\n",
            "\n",
            "PyTorch device: cuda\n",
            "\n",
            "============================================================\n",
            "PyTorch: Training depth=1\n",
            "============================================================\n",
            "  Parameters: 2,531,318\n",
            "  Epoch  1/20 | Train loss: 5.6676 | Dev loss: 4.1486 | Time: 4.7s\n",
            "  Epoch  2/20 | Train loss: 3.8118 | Dev loss: 3.5379 | Time: 3.4s\n",
            "  Epoch  3/20 | Train loss: 3.3453 | Dev loss: 3.2665 | Time: 3.5s\n",
            "  Epoch  4/20 | Train loss: 3.0581 | Dev loss: 3.0917 | Time: 4.2s\n",
            "  Epoch  5/20 | Train loss: 2.8285 | Dev loss: 2.9484 | Time: 3.5s\n",
            "  Epoch  6/20 | Train loss: 2.6360 | Dev loss: 2.8611 | Time: 3.4s\n",
            "  Epoch  7/20 | Train loss: 2.4631 | Dev loss: 2.7588 | Time: 3.8s\n",
            "  Epoch  8/20 | Train loss: 2.3032 | Dev loss: 2.6819 | Time: 3.8s\n",
            "  Epoch  9/20 | Train loss: 2.1538 | Dev loss: 2.6294 | Time: 3.4s\n",
            "  Epoch 10/20 | Train loss: 2.0117 | Dev loss: 2.5797 | Time: 3.4s\n",
            "  Epoch 11/20 | Train loss: 1.8728 | Dev loss: 2.5307 | Time: 4.1s\n",
            "  Epoch 12/20 | Train loss: 1.7387 | Dev loss: 2.5066 | Time: 3.5s\n",
            "  Epoch 13/20 | Train loss: 1.6100 | Dev loss: 2.4922 | Time: 3.4s\n",
            "  Epoch 14/20 | Train loss: 1.4873 | Dev loss: 2.4606 | Time: 3.7s\n",
            "  Epoch 15/20 | Train loss: 1.3699 | Dev loss: 2.4477 | Time: 3.8s\n",
            "  Epoch 16/20 | Train loss: 1.2591 | Dev loss: 2.4498 | Time: 3.4s\n",
            "  Epoch 17/20 | Train loss: 1.1539 | Dev loss: 2.4379 | Time: 3.4s\n",
            "  Epoch 18/20 | Train loss: 1.0553 | Dev loss: 2.4536 | Time: 4.2s\n",
            "  Epoch 19/20 | Train loss: 0.9619 | Dev loss: 2.4662 | Time: 3.4s\n",
            "  Epoch 20/20 | Train loss: 0.8750 | Dev loss: 2.4693 | Time: 3.4s\n",
            "\n",
            "  Test loss: 2.4024\n",
            "  Computing exact match on test set...\n",
            "  Exact match accuracy: 0.00%\n",
            "\n",
            "  >> depth=1: train=0.8750 | dev=2.4693 | test=2.4024 | exact_match=0.0%\n",
            "\n",
            "============================================================\n",
            "PyTorch: Training depth=2\n",
            "============================================================\n",
            "  Parameters: 2,729,590\n",
            "  Epoch  1/20 | Train loss: 5.6623 | Dev loss: 4.1653 | Time: 5.7s\n",
            "  Epoch  2/20 | Train loss: 3.8307 | Dev loss: 3.5356 | Time: 4.7s\n",
            "  Epoch  3/20 | Train loss: 3.3537 | Dev loss: 3.2541 | Time: 5.2s\n",
            "  Epoch  4/20 | Train loss: 3.0478 | Dev loss: 3.0836 | Time: 5.1s\n",
            "  Epoch  5/20 | Train loss: 2.8117 | Dev loss: 2.9529 | Time: 4.8s\n",
            "  Epoch  6/20 | Train loss: 2.6044 | Dev loss: 2.8541 | Time: 5.6s\n",
            "  Epoch  7/20 | Train loss: 2.4125 | Dev loss: 2.7592 | Time: 4.8s\n",
            "  Epoch  8/20 | Train loss: 2.2353 | Dev loss: 2.7053 | Time: 5.1s\n",
            "  Epoch  9/20 | Train loss: 2.0675 | Dev loss: 2.6634 | Time: 5.1s\n",
            "  Epoch 10/20 | Train loss: 1.9087 | Dev loss: 2.6235 | Time: 4.8s\n",
            "  Epoch 11/20 | Train loss: 1.7566 | Dev loss: 2.5936 | Time: 5.6s\n",
            "  Epoch 12/20 | Train loss: 1.6100 | Dev loss: 2.5830 | Time: 4.8s\n",
            "  Epoch 13/20 | Train loss: 1.4689 | Dev loss: 2.5655 | Time: 5.1s\n",
            "  Epoch 14/20 | Train loss: 1.3396 | Dev loss: 2.5604 | Time: 5.1s\n",
            "  Epoch 15/20 | Train loss: 1.2154 | Dev loss: 2.5552 | Time: 4.7s\n",
            "  Epoch 16/20 | Train loss: 1.0957 | Dev loss: 2.5647 | Time: 5.6s\n",
            "  Epoch 17/20 | Train loss: 0.9890 | Dev loss: 2.5760 | Time: 4.7s\n",
            "  Epoch 18/20 | Train loss: 0.8868 | Dev loss: 2.5930 | Time: 6.5s\n",
            "  Epoch 19/20 | Train loss: 0.7927 | Dev loss: 2.6285 | Time: 4.8s\n",
            "  Epoch 20/20 | Train loss: 0.7087 | Dev loss: 2.6420 | Time: 5.1s\n",
            "\n",
            "  Test loss: 2.5446\n",
            "  Computing exact match on test set...\n",
            "  Exact match accuracy: 0.00%\n",
            "\n",
            "  >> depth=2: train=0.7087 | dev=2.6420 | test=2.5446 | exact_match=0.0%\n",
            "\n",
            "============================================================\n",
            "PyTorch: Training depth=4\n",
            "============================================================\n",
            "  Parameters: 3,126,134\n",
            "  Epoch  1/20 | Train loss: 5.5984 | Dev loss: 4.0804 | Time: 8.0s\n",
            "  Epoch  2/20 | Train loss: 3.7597 | Dev loss: 3.4945 | Time: 8.6s\n",
            "  Epoch  3/20 | Train loss: 3.2995 | Dev loss: 3.2284 | Time: 7.5s\n",
            "  Epoch  4/20 | Train loss: 3.0004 | Dev loss: 3.0364 | Time: 8.3s\n",
            "  Epoch  5/20 | Train loss: 2.7555 | Dev loss: 2.9231 | Time: 8.5s\n",
            "  Epoch  6/20 | Train loss: 2.5416 | Dev loss: 2.8324 | Time: 7.5s\n",
            "  Epoch  7/20 | Train loss: 2.3483 | Dev loss: 2.7615 | Time: 8.5s\n",
            "  Epoch  8/20 | Train loss: 2.1708 | Dev loss: 2.7276 | Time: 8.4s\n",
            "  Epoch  9/20 | Train loss: 2.0042 | Dev loss: 2.6881 | Time: 7.6s\n",
            "  Epoch 10/20 | Train loss: 1.8478 | Dev loss: 2.6777 | Time: 9.2s\n",
            "  Epoch 11/20 | Train loss: 1.6965 | Dev loss: 2.6611 | Time: 8.2s\n",
            "  Epoch 12/20 | Train loss: 1.5500 | Dev loss: 2.6170 | Time: 7.7s\n",
            "  Epoch 13/20 | Train loss: 1.4085 | Dev loss: 2.6422 | Time: 8.5s\n",
            "  Epoch 14/20 | Train loss: 1.2771 | Dev loss: 2.6361 | Time: 7.7s\n",
            "  Epoch 15/20 | Train loss: 1.1485 | Dev loss: 2.6338 | Time: 8.2s\n",
            "  Epoch 16/20 | Train loss: 1.0277 | Dev loss: 2.6311 | Time: 8.5s\n",
            "  Epoch 17/20 | Train loss: 0.9156 | Dev loss: 2.6635 | Time: 7.6s\n",
            "  Epoch 18/20 | Train loss: 0.8103 | Dev loss: 2.6631 | Time: 8.5s\n",
            "  Epoch 19/20 | Train loss: 0.7154 | Dev loss: 2.7120 | Time: 8.5s\n",
            "  Epoch 20/20 | Train loss: 0.6271 | Dev loss: 2.7276 | Time: 7.6s\n",
            "\n",
            "  Test loss: 2.6532\n",
            "  Computing exact match on test set...\n",
            "  Exact match accuracy: 0.00%\n",
            "\n",
            "  >> depth=4: train=0.6271 | dev=2.7276 | test=2.6532 | exact_match=0.0%\n",
            "\n",
            "============================================================\n",
            "PyTorch: Training depth=8\n",
            "============================================================\n",
            "  Parameters: 3,919,222\n",
            "  Epoch  1/20 | Train loss: 5.6462 | Dev loss: 4.1408 | Time: 14.7s\n",
            "  Epoch  2/20 | Train loss: 3.8291 | Dev loss: 3.5586 | Time: 14.8s\n",
            "  Epoch  3/20 | Train loss: 3.3781 | Dev loss: 3.2765 | Time: 14.3s\n",
            "  Epoch  4/20 | Train loss: 3.0831 | Dev loss: 3.1151 | Time: 14.8s\n",
            "  Epoch  5/20 | Train loss: 2.8528 | Dev loss: 3.0027 | Time: 14.5s\n",
            "  Epoch  6/20 | Train loss: 2.6497 | Dev loss: 2.9148 | Time: 14.5s\n",
            "  Epoch  7/20 | Train loss: 2.4658 | Dev loss: 2.8620 | Time: 14.4s\n",
            "  Epoch  8/20 | Train loss: 2.2909 | Dev loss: 2.8321 | Time: 14.9s\n",
            "  Epoch  9/20 | Train loss: 2.1248 | Dev loss: 2.7995 | Time: 14.9s\n",
            "  Epoch 10/20 | Train loss: 1.9720 | Dev loss: 2.7958 | Time: 14.3s\n",
            "  Epoch 11/20 | Train loss: 1.8227 | Dev loss: 2.7928 | Time: 14.3s\n",
            "  Epoch 12/20 | Train loss: 1.6804 | Dev loss: 2.7792 | Time: 14.3s\n",
            "  Epoch 13/20 | Train loss: 1.5441 | Dev loss: 2.7879 | Time: 14.5s\n",
            "  Epoch 14/20 | Train loss: 1.4112 | Dev loss: 2.8091 | Time: 14.4s\n",
            "  Epoch 15/20 | Train loss: 1.2868 | Dev loss: 2.7973 | Time: 14.6s\n",
            "  Epoch 16/20 | Train loss: 1.1680 | Dev loss: 2.8516 | Time: 15.2s\n",
            "  Epoch 17/20 | Train loss: 1.0528 | Dev loss: 2.8262 | Time: 14.6s\n",
            "  Epoch 18/20 | Train loss: 0.9433 | Dev loss: 2.8811 | Time: 14.5s\n",
            "  Epoch 19/20 | Train loss: 0.8424 | Dev loss: 2.8939 | Time: 14.3s\n",
            "  Epoch 20/20 | Train loss: 0.7452 | Dev loss: 2.9093 | Time: 14.6s\n",
            "\n",
            "  Test loss: 2.8269\n",
            "  Computing exact match on test set...\n",
            "  Exact match accuracy: 0.00%\n",
            "\n",
            "  >> depth=8: train=0.7452 | dev=2.9093 | test=2.8269 | exact_match=0.0%\n",
            "\n",
            "============================================================\n",
            "PyTorch: Training depth=16\n",
            "============================================================\n",
            "  Parameters: 5,505,398\n",
            "  Epoch  1/20 | Train loss: 5.8238 | Dev loss: 4.5770 | Time: 28.3s\n",
            "  Epoch  2/20 | Train loss: 4.3563 | Dev loss: 4.1173 | Time: 29.3s\n",
            "  Epoch  3/20 | Train loss: 4.0024 | Dev loss: 3.8890 | Time: 28.9s\n",
            "  Epoch  4/20 | Train loss: 3.6915 | Dev loss: 3.5711 | Time: 28.8s\n",
            "  Epoch  5/20 | Train loss: 3.4531 | Dev loss: 3.4206 | Time: 27.9s\n",
            "  Epoch  6/20 | Train loss: 3.2795 | Dev loss: 3.2891 | Time: 27.6s\n",
            "  Epoch  7/20 | Train loss: 3.1270 | Dev loss: 3.2102 | Time: 27.7s\n",
            "  Epoch  8/20 | Train loss: 2.9938 | Dev loss: 3.1355 | Time: 27.5s\n",
            "  Epoch  9/20 | Train loss: 2.8618 | Dev loss: 3.0684 | Time: 27.6s\n",
            "  Epoch 10/20 | Train loss: 2.7381 | Dev loss: 3.0074 | Time: 28.3s\n",
            "  Epoch 11/20 | Train loss: 2.6240 | Dev loss: 2.9707 | Time: 27.6s\n",
            "  Epoch 12/20 | Train loss: 2.5094 | Dev loss: 2.9363 | Time: 27.8s\n",
            "  Epoch 13/20 | Train loss: 2.3937 | Dev loss: 2.9001 | Time: 28.0s\n",
            "  Epoch 14/20 | Train loss: 2.2848 | Dev loss: 2.8641 | Time: 27.7s\n",
            "  Epoch 15/20 | Train loss: 2.1761 | Dev loss: 2.8404 | Time: 28.0s\n",
            "  Epoch 16/20 | Train loss: 2.0744 | Dev loss: 2.8209 | Time: 28.0s\n",
            "  Epoch 17/20 | Train loss: 1.9724 | Dev loss: 2.8667 | Time: 27.9s\n",
            "  Epoch 18/20 | Train loss: 1.8787 | Dev loss: 2.8318 | Time: 28.2s\n",
            "  Epoch 19/20 | Train loss: 1.7840 | Dev loss: 2.8375 | Time: 28.2s\n",
            "  Epoch 20/20 | Train loss: 1.6911 | Dev loss: 2.8123 | Time: 28.2s\n",
            "\n",
            "  Test loss: 2.7525\n",
            "  Computing exact match on test set...\n",
            "  Exact match accuracy: 0.00%\n",
            "\n",
            "  >> depth=16: train=1.6911 | dev=2.8123 | test=2.7525 | exact_match=0.0%\n"
          ]
        }
      ],
      "source": [
        "# Run the experiment\n",
        "print(\"=\"*60)\n",
        "print(\"PYTORCH EXPERIMENT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "pytorch_results = run_pytorch_experiment(\n",
        "    vocab, tokenizer, train_dataset, dev_dataset, test_dataset, PYTORCH_CONFIG\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3tYSQQn0tFc"
      },
      "outputs": [],
      "source": [
        "# Save and visualize results\n",
        "save_results(pytorch_results, os.path.join(OUTPUT_DIR, 'pytorch_results.json'))\n",
        "\n",
        "plot_depth_comparison(\n",
        "    pytorch_results, 'PyTorch', PYTORCH_CONFIG,\n",
        "    os.path.join(OUTPUT_DIR, 'pytorch_depth_comparison.png')\n",
        ")\n",
        "\n",
        "plot_final_bar_chart(\n",
        "    pytorch_results, 'PyTorch', PYTORCH_CONFIG,\n",
        "    os.path.join(OUTPUT_DIR, 'pytorch_final_bar_chart.png')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G8eDIVv0tFc"
      },
      "source": [
        "## 8. Run TorchLite Experiment\n",
        "\n",
        "⚠️ TorchLite uses NumPy-based autograd. Much slower but educational! This takes ~5 minutes for depth=1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hQxGJLR0tFc"
      },
      "outputs": [],
      "source": [
        "# Import TorchLite training functions\n",
        "from torchlite.optim import Adam\n",
        "from model.transformer_lite import TransformerDecoderLite\n",
        "from torchlite.tensor import Tensor\n",
        "\n",
        "def compute_masked_loss_lite(logits, targets, loss_mask):\n",
        "    \"\"\"\n",
        "    Compute cross-entropy loss for TorchLite tensors with masking.\n",
        "    \"\"\"\n",
        "    batch_size, seq_len, vocab_size = logits.shape\n",
        "\n",
        "    # Softmax over vocab dimension\n",
        "    probs = logits.softmax(axis=-1)\n",
        "\n",
        "    # Create one-hot targets\n",
        "    targets_flat = targets.reshape(-1)\n",
        "    one_hot = np.zeros((batch_size * seq_len, vocab_size), dtype=np.float64)\n",
        "    one_hot[np.arange(batch_size * seq_len), targets_flat] = 1.0\n",
        "\n",
        "    # Reshape probs to [batch*seq, vocab]\n",
        "    probs_flat = probs.reshape(batch_size * seq_len, vocab_size)\n",
        "\n",
        "    # Gather: sum(probs * one_hot, axis=-1) gives prob at target index\n",
        "    target_probs = (probs_flat * Tensor(one_hot)).sum(axis=-1)\n",
        "    neg_log_probs = target_probs.log() * Tensor(-1.0)\n",
        "\n",
        "    # Apply mask\n",
        "    mask_flat = Tensor(loss_mask.reshape(-1).astype(np.float64))\n",
        "    masked_losses = neg_log_probs * mask_flat\n",
        "\n",
        "    # Average over masked tokens\n",
        "    n_masked = float(loss_mask.sum()) + 1e-8\n",
        "    loss = masked_losses.sum() * Tensor(1.0 / n_masked)\n",
        "\n",
        "    return loss\n",
        "\n",
        "print(\"✓ TorchLite loss function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNJJgGsM0tFc"
      },
      "outputs": [],
      "source": [
        "def train_epoch_lite(model, dataloader, optimizer, n_batches, epoch):\n",
        "    \"\"\"Train TorchLite model for one epoch.\"\"\"\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        if batch_idx >= n_batches:\n",
        "            break\n",
        "\n",
        "        input_ids = batch['input_ids']\n",
        "        target_ids = batch['target_ids']\n",
        "        loss_mask = batch['loss_mask']\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model.forward(input_ids)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = compute_masked_loss_lite(logits, target_ids, loss_mask)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate metrics\n",
        "        n_tokens = float(loss_mask.sum())\n",
        "        total_loss += loss.item() * n_tokens\n",
        "        total_tokens += n_tokens\n",
        "\n",
        "        if (batch_idx + 1) % 5 == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            avg_loss = total_loss / total_tokens\n",
        "            print(f\"    Batch {batch_idx + 1}/{n_batches} | Loss: {avg_loss:.4f} | {elapsed:.1f}s\")\n",
        "\n",
        "    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')\n",
        "    return {'loss': avg_loss, 'time': time.time() - start_time}\n",
        "\n",
        "print(\"✓ TorchLite training function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eo59O8g40tFd"
      },
      "outputs": [],
      "source": [
        "def evaluate_lite(model, dataloader, n_batches):\n",
        "    \"\"\"Evaluate TorchLite model.\"\"\"\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        if batch_idx >= n_batches:\n",
        "            break\n",
        "\n",
        "        input_ids = batch['input_ids']\n",
        "        target_ids = batch['target_ids']\n",
        "        loss_mask = batch['loss_mask']\n",
        "\n",
        "        logits = model.forward(input_ids)\n",
        "        loss = compute_masked_loss_lite(logits, target_ids, loss_mask)\n",
        "\n",
        "        n_tokens = float(loss_mask.sum())\n",
        "        total_loss += loss.item() * n_tokens\n",
        "        total_tokens += n_tokens\n",
        "\n",
        "    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')\n",
        "    return {'loss': avg_loss}\n",
        "\n",
        "print(\"✓ TorchLite evaluation function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pIiIqKG0tFd"
      },
      "outputs": [],
      "source": [
        "def run_torchlite_experiment(vocab, tokenizer, train_dataset, dev_dataset, test_dataset, config):\n",
        "    \"\"\"Train TorchLite models and return loss histories + final metrics.\"\"\"\n",
        "    results = {}\n",
        "    n_train_batches = len(train_dataset) // config['batch_size']\n",
        "    n_dev_batches = max(1, len(dev_dataset) // config['batch_size'])\n",
        "    n_test_batches = max(1, len(test_dataset) // config['batch_size'])\n",
        "\n",
        "    for depth in config['depths']:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"TorchLite: Training depth={depth}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        model = TransformerDecoderLite(\n",
        "            vocab_size=len(vocab),\n",
        "            d_model=config['d_model'],\n",
        "            n_heads=config['n_heads'],\n",
        "            n_layers=depth,\n",
        "            d_ff=config['d_ff'],\n",
        "            max_len=config['max_len'],\n",
        "        )\n",
        "\n",
        "        n_params = sum(p.data.size for p in model.parameters())\n",
        "        print(f\"  Parameters: {n_params:,}\")\n",
        "\n",
        "        optimizer = Adam(model.parameters(), lr=config['lr'])\n",
        "\n",
        "        train_losses = []\n",
        "        dev_losses = []\n",
        "\n",
        "        for epoch in range(1, config['n_epochs'] + 1):\n",
        "            print(f\"\\n  Epoch {epoch}/{config['n_epochs']}\")\n",
        "\n",
        "            train_loader = create_torchlite_dataloader(\n",
        "                train_dataset, config['batch_size'], shuffle=True, pad_id=vocab.pad_id\n",
        "            )\n",
        "            dev_loader = create_torchlite_dataloader(\n",
        "                dev_dataset, config['batch_size'], shuffle=False, pad_id=vocab.pad_id\n",
        "            )\n",
        "\n",
        "            train_metrics = train_epoch_lite(\n",
        "                model, train_loader, optimizer, n_train_batches, epoch\n",
        "            )\n",
        "            dev_metrics = evaluate_lite(model, dev_loader, n_dev_batches)\n",
        "\n",
        "            train_losses.append(train_metrics['loss'])\n",
        "            dev_losses.append(dev_metrics['loss'])\n",
        "\n",
        "            print(f\"  Epoch {epoch} | \"\n",
        "                  f\"Train loss: {train_metrics['loss']:.4f} | \"\n",
        "                  f\"Dev loss: {dev_metrics['loss']:.4f} | \"\n",
        "                  f\"Time: {train_metrics['time']:.1f}s\")\n",
        "\n",
        "        # Final evaluation on test set\n",
        "        test_loader = create_torchlite_dataloader(\n",
        "            test_dataset, config['batch_size'], shuffle=False, pad_id=vocab.pad_id\n",
        "        )\n",
        "        test_metrics = evaluate_lite(model, test_loader, n_test_batches)\n",
        "        print(f\"\\n  Test loss: {test_metrics['loss']:.4f}\")\n",
        "\n",
        "        # Exact match accuracy on test set\n",
        "        print(f\"  Computing exact match on test set...\")\n",
        "        exact_match = compute_exact_match_lite(model, test_dataset, tokenizer, num_samples=20)\n",
        "        print(f\"  Exact match accuracy: {exact_match:.2%}\")\n",
        "\n",
        "        results[depth] = {\n",
        "            'train_losses': train_losses,\n",
        "            'dev_losses': dev_losses,\n",
        "            'final_train_loss': train_losses[-1],\n",
        "            'final_dev_loss': dev_losses[-1],\n",
        "            'final_test_loss': test_metrics['loss'],\n",
        "            'exact_match': exact_match,\n",
        "            'n_params': n_params,\n",
        "        }\n",
        "\n",
        "        print(f\"\\n  >> depth={depth}: \"\n",
        "              f\"train={train_losses[-1]:.4f} | \"\n",
        "              f\"dev={dev_losses[-1]:.4f} | \"\n",
        "              f\"test={test_metrics['loss']:.4f} | \"\n",
        "              f\"exact_match={exact_match:.1%}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"✓ TorchLite experiment function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8a8Q3vM0tFd"
      },
      "outputs": [],
      "source": [
        "# Run the TorchLite experiment\n",
        "print(\"=\"*60)\n",
        "print(\"TORCHLITE EXPERIMENT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "torchlite_results = run_torchlite_experiment(\n",
        "    vocab, tokenizer, train_dataset, dev_dataset, test_dataset, TORCHLITE_CONFIG\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eJzXfPn0tFd"
      },
      "outputs": [],
      "source": [
        "# Save and visualize TorchLite results\n",
        "save_results(torchlite_results, os.path.join(OUTPUT_DIR, 'torchlite_results.json'))\n",
        "\n",
        "plot_depth_comparison(\n",
        "    torchlite_results, 'TorchLite', TORCHLITE_CONFIG,\n",
        "    os.path.join(OUTPUT_DIR, 'torchlite_depth_comparison.png')\n",
        ")\n",
        "\n",
        "plot_final_bar_chart(\n",
        "    torchlite_results, 'TorchLite', TORCHLITE_CONFIG,\n",
        "    os.path.join(OUTPUT_DIR, 'torchlite_final_bar_chart.png')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OJ0WKR80tFd"
      },
      "source": [
        "## 9. Summary & Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJlpUMOF0tFe"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPERIMENT COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nResults saved to: {OUTPUT_DIR}\")\n",
        "print(\"\\nGenerated files:\")\n",
        "for f in os.listdir(OUTPUT_DIR):\n",
        "    path = os.path.join(OUTPUT_DIR, f)\n",
        "    if os.path.isfile(path):\n",
        "        size = os.path.getsize(path) / 1024\n",
        "        print(f\"  - {f} ({size:.1f} KB)\")\n",
        "\n",
        "print(\"\\n✓ All experiments completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bNm0H1K0tFe"
      },
      "outputs": [],
      "source": [
        "# Optional: Copy results to Google Drive\n",
        "# import shutil\n",
        "# drive_dir = '/content/drive/MyDrive/nl2bash_results'\n",
        "# shutil.copytree(OUTPUT_DIR, drive_dir, dirs_exist_ok=True)\n",
        "# print(f\"Results copied to Drive: {drive_dir}\")\n",
        "\n",
        "print(\"To save results to Drive, uncomment the cell above and update the path.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oqb0m2jU26GH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "qZWC_MgV0tFV",
        "8nVJH_cK0tFX",
        "p3S_Yf5_0tFZ",
        "2z_yqIVT0tFb"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}