# PyTorch vs TorchLite Implementation Comparison

This document compares the two transformer implementations for educational purposes.

---

## Overview

| Feature | PyTorch | TorchLite |
|---------|---------|-----------|
| **File** | `model/transformer_full.py` | `model/transformer_lite.py` |
| **Parameters** | ~9.7M | ~3.4M |
| **Model size** | d_model=256, n_layers=6 | d_model=128, n_layers=3 |
| **Speed** | Fast (optimized CUDA/CPU) | Slow (pure Python/NumPy) |
| **Purpose** | Production training | Educational understanding |
| **Lines of code** | ~471 | ~567 |

---

## Architecture Comparison

### PyTorch Version
```python
class TransformerDecoder(nn.Module):
    def __init__(self, vocab_size, d_model=256, n_heads=4, n_layers=6):
        # Token embedding (nn.Embedding)
        self.embed = nn.Embedding(vocab_size, d_model)

        # Positional encoding (registered buffer)
        self.register_buffer('pos_encoding', ...)

        # Decoder layers (nn.ModuleList)
        self.layers = nn.ModuleList([
            DecoderLayer(d_model, n_heads, d_ff, dropout)
            for _ in range(n_layers)
        ])

        # Output projection
        self.output = nn.Linear(d_model, vocab_size)

    def forward(self, x, mask=None):
        # Uses PyTorch autograd automatically
        ...
```

### TorchLite Version
```python
class TransformerDecoderLite(Module):
    def __init__(self, vocab_size, d_model=128, n_heads=4, n_layers=3):
        # Token embedding (custom Embedding class)
        self.embed = Embedding(vocab_size, d_model)

        # Positional encoding (numpy array)
        self.pos_encoding = create_positional_encoding(max_len, d_model)

        # Decoder layers (Python list)
        self.layers = [
            DecoderLayer(d_model, n_heads, d_ff)
            for _ in range(n_layers)
        ]

        # Output projection
        self.output = Linear(d_model, vocab_size)

    def forward(self, x, mask=None):
        # Manual operations with Tensor objects
        ...
```

---

## Key Differences

### 1. Attention Implementation

**PyTorch** (automatic, optimized):
```python
# Multi-head attention in one line
attn_output = F.scaled_dot_product_attention(Q, K, V, attn_mask=mask)
```

**TorchLite** (explicit, educational):
```python
# Step-by-step attention computation
scores = np.matmul(Q_data, K_data.transpose(0, 1, 3, 2)) / math.sqrt(self.d_k)
scores = scores + mask[np.newaxis, np.newaxis, :, :]
scores_exp = np.exp(scores - np.max(scores, axis=-1, keepdims=True))
attn_weights = scores_exp / np.sum(scores_exp, axis=-1, keepdims=True)
attn_output = np.matmul(attn_weights, V_data)
```

**Learning value**: TorchLite shows **exactly** what happens in attention!

---

### 2. Softmax Function

**PyTorch**:
```python
probs = F.softmax(logits, dim=-1)
```

**TorchLite**:
```python
def softmax(x, axis=-1):
    x_max = np.max(x.data, axis=axis, keepdims=True)
    exp_x = np.exp(x.data - x_max)
    sum_exp_x = np.sum(exp_x, axis=axis, keepdims=True)
    return Tensor(exp_x / sum_exp_x, requires_grad=x.requires_grad)
```

**Learning value**: See numerical stability trick (subtract max)

---

### 3. Layer Normalization

**PyTorch**:
```python
self.norm = nn.LayerNorm(d_model)
x = self.norm(x)
```

**TorchLite**:
```python
def layer_norm(x, eps=1e-5):
    mean = np.mean(x.data, axis=-1, keepdims=True)
    var = np.var(x.data, axis=-1, keepdims=True)
    normalized = (x.data - mean) / np.sqrt(var + eps)
    return Tensor(normalized, requires_grad=x.requires_grad)
```

**Note**: TorchLite version currently skips layer norm in decoder for simplicity (uses residuals only)

---

### 4. Embedding Layer

**PyTorch**:
```python
self.embed = nn.Embedding(vocab_size, d_model)
x = self.embed(token_ids)  # Automatic lookup + gradient tracking
```

**TorchLite**:
```python
class Embedding(Module):
    def __init__(self, vocab_size, d_model):
        self.weight = Tensor(
            np.random.randn(vocab_size, d_model) * scale,
            requires_grad=True
        )

    def forward(self, indices):
        embeddings = self.weight.data[indices]  # Manual lookup
        return Tensor(embeddings, requires_grad=True)
```

**Learning value**: Embedding is just a lookup table!

---

### 5. Dropout

**PyTorch**:
```python
self.dropout = nn.Dropout(dropout)
x = self.dropout(x)  # Randomly zeroes elements during training
```

**TorchLite**:
```python
# Not implemented (skipped for simplicity)
```

**Note**: TorchLite omits dropout to keep code simple. Could add as:
```python
if training:
    mask = np.random.binomial(1, 1-dropout, x.shape) / (1-dropout)
    x = x * mask
```

---

### 6. Generation

Both use greedy decoding, but with different APIs:

**PyTorch**:
```python
generated = model.generate(
    nl_tokens,
    start_id=2,
    end_id=3,
    max_len=64,
    temperature=1.0,    # Sampling temperature
    top_k=50            # Top-k sampling
)
```

**TorchLite**:
```python
generated = model.generate(
    nl_tokens,
    start_id=2,
    end_id=3,
    max_len=64
    # No temperature or top_k (simplified)
)
```

---

## Code Complexity Comparison

### Multi-Head Attention

**PyTorch** (~30 lines):
- Uses `nn.Linear` for projections
- Automatic gradient tracking
- Optimized matmul operations

**TorchLite** (~70 lines):
- Manual Linear layer calls
- Explicit reshaping and transposing
- NumPy matmul operations
- Shows every step

### Decoder Layer

**PyTorch** (~25 lines):
```python
class DecoderLayer(nn.Module):
    def forward(self, x, mask=None):
        # Self-attention + residual + norm
        attn_output = self.self_attn(x, mask)
        x = self.norm1(x + self.dropout1(attn_output))

        # Feed-forward + residual + norm
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout2(ff_output))

        return x
```

**TorchLite** (~20 lines):
```python
class DecoderLayer(Module):
    def forward(self, x, mask=None):
        # Self-attention + residual (no norm, no dropout)
        attn_output = self.self_attn(x, mask)
        x_data = x.data + attn_output.data
        x = Tensor(x_data, requires_grad=True)

        # Feed-forward + residual
        ff_output = self.feed_forward(x)
        x_data = x.data + ff_output.data
        x = Tensor(x_data, requires_grad=True)

        return x
```

**Difference**: PyTorch has layer norm and dropout; TorchLite skips them for simplicity.

---

## Performance Comparison

### Training Speed (estimated)

| Operation | PyTorch (GPU) | PyTorch (CPU) | TorchLite (CPU) |
|-----------|---------------|---------------|-----------------|
| Forward pass | 5-10ms | 50-100ms | 500-1000ms |
| Backward pass | 10-20ms | 100-200ms | N/A (manual) |
| Epoch | 5-10min | 2-3hrs | 20-30hrs |
| Full training | 5-8hrs | 100-150hrs | 1000-1500hrs |

**Conclusion**: PyTorch is **100-200x faster** than TorchLite!

### Memory Usage

| Model | Parameters | Memory |
|-------|------------|--------|
| PyTorch | 9.7M | ~40MB |
| TorchLite | 3.4M | ~14MB |

---

## When to Use Each

### Use PyTorch When:
- ‚úÖ You want to actually **train** the model
- ‚úÖ You need **fast** training (GPU support)
- ‚úÖ You want **production-ready** code
- ‚úÖ You need advanced features (temperature, top-k, dropout)

### Use TorchLite When:
- ‚úÖ You want to **understand** how transformers work
- ‚úÖ You're **learning** about attention mechanisms
- ‚úÖ You want to see **every operation explicitly**
- ‚úÖ You're teaching or building intuition
- ‚úÖ You want to **modify** the architecture easily

---

## Educational Value

### What TorchLite Teaches You

1. **Attention is just matrix multiplication**
   ```python
   scores = Q @ K.T / sqrt(d_k)
   attn_weights = softmax(scores)
   output = attn_weights @ V
   ```

2. **Embeddings are lookup tables**
   ```python
   embeddings = weight_matrix[token_ids]
   ```

3. **Positional encoding uses sin/cos**
   ```python
   PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
   PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
   ```

4. **Causal masking is just adding -inf**
   ```python
   mask = upper_triangular_matrix * -inf
   scores = scores + mask
   ```

5. **Generation is iterative**
   ```python
   for _ in range(max_len):
       logits = model(sequence)
       next_token = argmax(logits[-1])
       sequence.append(next_token)
   ```

### What You Learn by Comparing

- **Abstraction vs Explicitness**: PyTorch hides complexity, TorchLite shows it
- **Performance vs Clarity**: Fast code is harder to understand
- **Libraries vs Implementation**: When to use libraries vs build from scratch
- **Production vs Education**: Different goals need different approaches

---

## Code Example: Attention

### PyTorch (Abstracted)
```python
def forward(self, x, mask=None):
    Q = self.W_q(x)
    K = self.W_k(x)
    V = self.W_v(x)

    # Magic happens here (optimized C++ code)
    attn_output = F.scaled_dot_product_attention(Q, K, V, attn_mask=mask)

    return self.W_o(attn_output)
```

### TorchLite (Explicit)
```python
def forward(self, x, mask=None):
    Q = self.W_q(x)
    K = self.W_k(x)
    V = self.W_v(x)

    # Reshape for multi-head
    Q_data = Q.data.reshape(batch, seq, heads, d_k).transpose(0, 2, 1, 3)
    K_data = K.data.reshape(batch, seq, heads, d_k).transpose(0, 2, 1, 3)
    V_data = V.data.reshape(batch, seq, heads, d_k).transpose(0, 2, 1, 3)

    # Compute attention scores
    scores = np.matmul(Q_data, K_data.transpose(0, 1, 3, 2))
    scores = scores / math.sqrt(self.d_k)

    # Apply mask
    if mask is not None:
        scores = scores + mask[np.newaxis, np.newaxis, :, :]

    # Softmax
    scores_exp = np.exp(scores - np.max(scores, axis=-1, keepdims=True))
    attn_weights = scores_exp / np.sum(scores_exp, axis=-1, keepdims=True)

    # Apply to values
    attn_output = np.matmul(attn_weights, V_data)

    # Reshape back
    attn_output = attn_output.transpose(0, 2, 1, 3)
    attn_output = attn_output.reshape(batch, seq, d_model)

    return self.W_o(Tensor(attn_output, requires_grad=True))
```

**Observation**: TorchLite shows **every step** - you can trace exactly what's happening!

---

## Recommendation

1. **Start with TorchLite**: Understand the mechanics
2. **Then use PyTorch**: Train the actual model
3. **Compare results**: See how optimizations affect performance

Both implementations produce the same results (given same weights). The difference is:
- **PyTorch**: "Just train it"
- **TorchLite**: "Here's how it works"

---

## Summary

| Aspect | PyTorch | TorchLite |
|--------|---------|-----------|
| **Speed** | üöÄüöÄüöÄüöÄüöÄ Fast | üê¢ Slow |
| **Clarity** | üòï Abstract | üí° Explicit |
| **Features** | ‚úÖ Complete | üîß Basic |
| **Learning** | üìö Use library | üéì Understand internals |
| **Production** | ‚úÖ Yes | ‚ùå No |
| **Education** | üòê Okay | ‚úÖ Excellent |

**Bottom line**: Use PyTorch for training, use TorchLite for learning!
