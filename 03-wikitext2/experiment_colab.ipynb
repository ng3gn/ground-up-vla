{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# WikiText-2 Transformer Scaling Experiments (PyTorch)\n\nThis notebook trains decoder-only Transformers on WikiText-2 and compares\nperformance as a function of several independent variables:\n\n1. **Depth** -- varying number of transformer blocks (fixed width)\n2. **Width** -- varying d_model (fixed depth)\n3. **Batch Size** -- varying training batch size\n4. **Dropout** -- varying dropout rate\n5. **L2 Regularization** -- varying weight decay\n6. **L1 Regularization** -- varying L1 penalty strength\n7. **Elastic Net (Ratio)** -- varying L1/L2 ratio at fixed total strength\n8. **Elastic Net (Strength)** -- varying total strength at fixed ratio\n\nFor each experiment we produce loss curve plots comparing the settings.\n\nEverything is self-contained -- no external project files needed."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q matplotlib numpy kagglehub\n",
    "\n",
    "print('Dependencies installed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: mount Google Drive for persistent outputs\n",
    "MOUNT_DRIVE = False\n",
    "\n",
    "if MOUNT_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print('Google Drive mounted')\n",
    "else:\n",
    "    print('Drive mount skipped (set MOUNT_DRIVE=True to enable)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download WikiText-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "\n",
    "WIKITEXT2_DIR = kagglehub.dataset_download('vivekmettu/wikitext2-data')\n",
    "\n",
    "required = {'wiki.train.tokens', 'wiki.valid.tokens', 'wiki.test.tokens'}\n",
    "if not required.issubset(set(os.listdir(WIKITEXT2_DIR))):\n",
    "    # Some Kaggle datasets place files in a nested subfolder.\n",
    "    found = None\n",
    "    for root, _, files in os.walk(WIKITEXT2_DIR):\n",
    "        if required.issubset(set(files)):\n",
    "            found = root\n",
    "            break\n",
    "    if found is None:\n",
    "        raise FileNotFoundError('Could not find wiki.train/valid/test.tokens in downloaded dataset')\n",
    "    WIKITEXT2_DIR = found\n",
    "\n",
    "print('Path to dataset files:', WIKITEXT2_DIR)\n",
    "for fname in sorted(required):\n",
    "    path = os.path.join(WIKITEXT2_DIR, fname)\n",
    "    size_mb = os.path.getsize(path) / 1024 / 1024\n",
    "    print(f'  - {fname} ({size_mb:.2f} MB)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Imports and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nimport time\nimport math\nfrom collections import Counter\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\n\n\n# ---------------------------------------------------------------------------\n# Decoder-only Transformer (GPT-style) -- inlined for self-contained notebook\n# ---------------------------------------------------------------------------\n\ndef create_positional_encoding(max_len, d_model):\n    pe = torch.zeros(max_len, d_model)\n    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n    div_term = torch.exp(\n        torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n    )\n    pe[:, 0::2] = torch.sin(position * div_term)\n    pe[:, 1::2] = torch.cos(position * div_term)\n    return pe\n\n\ndef create_causal_mask(seq_len, device):\n    mask = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1)\n    mask = mask.masked_fill(mask == 1, float('-inf'))\n    return mask\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, n_heads, dropout=0.1):\n        super().__init__()\n        assert d_model % n_heads == 0\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        B, S, D = x.size()\n        Q = self.W_q(x).view(B, S, self.n_heads, self.d_k).transpose(1, 2)\n        K = self.W_k(x).view(B, S, self.n_heads, self.d_k).transpose(1, 2)\n        V = self.W_v(x).view(B, S, self.n_heads, self.d_k).transpose(1, 2)\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        if mask is not None:\n            scores = scores + mask.unsqueeze(0).unsqueeze(0)\n        attn = self.dropout(F.softmax(scores, dim=-1))\n        out = torch.matmul(attn, V)\n        out = out.transpose(1, 2).contiguous().view(B, S, D)\n        return self.W_o(out)\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        x = self.norm1(x + self.dropout1(self.self_attn(x, mask)))\n        x = self.norm2(x + self.dropout2(self.feed_forward(x)))\n        return x\n\n\nclass TransformerDecoder(nn.Module):\n    def __init__(self, vocab_size, d_model=256, n_heads=4, n_layers=6,\n                 d_ff=1024, dropout=0.1, max_len=128):\n        super().__init__()\n        self.d_model = d_model\n        self.embed = nn.Embedding(vocab_size, d_model)\n        self.register_buffer('pos_encoding',\n                             create_positional_encoding(max_len, d_model))\n        self.layers = nn.ModuleList([\n            DecoderLayer(d_model, n_heads, d_ff, dropout)\n            for _ in range(n_layers)\n        ])\n        self.output = nn.Linear(d_model, vocab_size)\n        self.dropout = nn.Dropout(dropout)\n        self._init_weights()\n\n    def _init_weights(self):\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n\n    def forward(self, x, mask=None):\n        B, S = x.size()\n        if mask is None:\n            mask = create_causal_mask(S, x.device)\n        x = self.embed(x) * math.sqrt(self.d_model)\n        x = x + self.pos_encoding[:S, :].unsqueeze(0)\n        x = self.dropout(x)\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.output(x)\n\n\nprint('Modules imported and TransformerDecoder defined')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "CONFIG = {\n    # Fixed model defaults\n    'd_model': 256,\n    'n_heads': 4,\n    'n_layers': 4,\n    'd_ff': 1024,\n    'dropout': 0.1,\n    'max_len': 128,\n\n    # Training\n    'batch_size': 32,\n    'lr': 1e-2,\n    'momentum': 0.0,\n    'weight_decay': 0.0,\n    'n_epochs': 8,\n    'grad_clip': 1.0,\n\n    # Data\n    'max_vocab_size': 20000,\n    'min_freq': 2,\n    'max_train_tokens': 1200000,\n\n    # Experiment sweeps\n    'depths': [1, 2, 4, 8],                # depth experiment: vary n_layers\n    'widths': [64, 128, 256, 512],          # width experiment: vary d_model (must be divisible by n_heads)\n\n    # Base model for regularization experiments\n    'reg_n_layers': 4,\n    'reg_d_model': 256,\n    'reg_d_ff': 1024,\n\n    # Regularization sweeps\n    'dropout_rates': [0.0, 0.1, 0.2, 0.3, 0.5],\n    'l2_weights': [0.0, 0.001, 0.01, 0.1],\n    'l1_weights': [0.0, 0.0001, 0.001, 0.01],\n    'enet_total_strength': 0.01,              # fixed strength for ratio sweep\n    'enet_ratios': [0.0, 0.25, 0.5, 0.75, 1.0],  # 0=pure L2, 1=pure L1\n    'enet_ratio': 0.5,                        # fixed ratio for strength sweep\n    'enet_strengths': [0.0, 0.001, 0.01, 0.1],\n    'batch_sizes': [8, 16, 32, 64, 128],\n}\n\nOUTPUT_DIR = '/content/experiment_outputs_wikitext2'\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint('Configuration loaded')\nprint('Depth sweep:', CONFIG['depths'])\nprint('Width sweep:', CONFIG['widths'])\nprint('Dropout sweep:', CONFIG['dropout_rates'])\nprint('L2 sweep:', CONFIG['l2_weights'])\nprint('L1 sweep:', CONFIG['l1_weights'])\nprint('Batch size sweep:', CONFIG['batch_sizes'])\nprint('Output dir:', OUTPUT_DIR)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build WikiText-2 Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wikitext_split(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = [line.strip() for line in f]\n",
    "    return [line for line in lines if line]\n",
    "\n",
    "\n",
    "def build_vocab(train_lines, max_vocab_size=20000, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for line in train_lines:\n",
    "        counter.update(line.split())\n",
    "\n",
    "    specials = ['<pad>', '<unk>', '<eos>']\n",
    "    tokens = [tok for tok, freq in counter.most_common() if freq >= min_freq]\n",
    "    tokens = tokens[: max_vocab_size - len(specials)]\n",
    "    itos = specials + tokens\n",
    "    stoi = {tok: i for i, tok in enumerate(itos)}\n",
    "    return stoi, itos\n",
    "\n",
    "\n",
    "def encode_lines(lines, stoi):\n",
    "    unk_id = stoi['<unk>']\n",
    "    eos_id = stoi['<eos>']\n",
    "    ids = []\n",
    "    for line in lines:\n",
    "        ids.extend(stoi.get(tok, unk_id) for tok in line.split())\n",
    "        ids.append(eos_id)\n",
    "    return ids\n",
    "\n",
    "\n",
    "def make_lm_blocks(token_ids, seq_len):\n",
    "    n_blocks = (len(token_ids) - 1) // seq_len\n",
    "    if n_blocks <= 0:\n",
    "        raise ValueError('Not enough tokens to create at least one block')\n",
    "\n",
    "    trim = n_blocks * seq_len + 1\n",
    "    arr = torch.tensor(token_ids[:trim], dtype=torch.long)\n",
    "    x = arr[:-1].view(n_blocks, seq_len)\n",
    "    y = arr[1:].view(n_blocks, seq_len)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "train_lines = read_wikitext_split(os.path.join(WIKITEXT2_DIR, 'wiki.train.tokens'))\n",
    "valid_lines = read_wikitext_split(os.path.join(WIKITEXT2_DIR, 'wiki.valid.tokens'))\n",
    "test_lines = read_wikitext_split(os.path.join(WIKITEXT2_DIR, 'wiki.test.tokens'))\n",
    "\n",
    "stoi, itos = build_vocab(\n",
    "    train_lines,\n",
    "    max_vocab_size=CONFIG['max_vocab_size'],\n",
    "    min_freq=CONFIG['min_freq'],\n",
    ")\n",
    "\n",
    "train_ids = encode_lines(train_lines, stoi)\n",
    "valid_ids = encode_lines(valid_lines, stoi)\n",
    "test_ids = encode_lines(test_lines, stoi)\n",
    "\n",
    "if CONFIG['max_train_tokens'] is not None:\n",
    "    train_ids = train_ids[:CONFIG['max_train_tokens']]\n",
    "\n",
    "train_x, train_y = make_lm_blocks(train_ids, CONFIG['max_len'])\n",
    "valid_x, valid_y = make_lm_blocks(valid_ids, CONFIG['max_len'])\n",
    "test_x, test_y = make_lm_blocks(test_ids, CONFIG['max_len'])\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_x, train_y), batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "valid_loader = DataLoader(TensorDataset(valid_x, valid_y), batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(test_x, test_y), batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "print('WikiText-2 prepared')\n",
    "print('Vocab size:', len(itos))\n",
    "print('Train blocks:', len(train_x))\n",
    "print('Valid blocks:', len(valid_x))\n",
    "print('Test blocks:', len(test_x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_lm(model, dataloader, device):\n    model.eval()\n    total_loss = 0.0\n    total_tokens = 0\n    total_correct = 0\n\n    with torch.no_grad():\n        for x, y in dataloader:\n            x = x.to(device)\n            y = y.to(device)\n            logits = model(x)\n\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), reduction='sum')\n            total_loss += loss.item()\n\n            preds = logits.argmax(dim=-1)\n            total_correct += (preds == y).sum().item()\n            total_tokens += y.numel()\n\n    avg_loss = total_loss / max(total_tokens, 1)\n    ppl = float(np.exp(avg_loss))\n    acc = total_correct / max(total_tokens, 1)\n\n    return {'loss': avg_loss, 'perplexity': ppl, 'token_acc': acc}\n\n\ndef l1_penalty(model):\n    \"\"\"Sum of absolute values of all model parameters (for L1 regularization).\"\"\"\n    return sum(p.abs().sum() for p in model.parameters())\n\n\ndef train_epoch_lm(model, dataloader, optimizer, device, grad_clip=1.0, l1_lambda=0.0):\n    model.train()\n    total_loss = 0.0\n    total_tokens = 0\n    t0 = time.time()\n\n    for x, y in dataloader:\n        x = x.to(device)\n        y = y.to(device)\n\n        optimizer.zero_grad()\n        logits = model(x)\n        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), reduction='mean')\n\n        if l1_lambda > 0:\n            loss = loss + l1_lambda * l1_penalty(model)\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n        optimizer.step()\n\n        total_loss += loss.item() * y.numel()\n        total_tokens += y.numel()\n\n    avg_loss = total_loss / max(total_tokens, 1)\n    return {\n        'loss': avg_loss,\n        'perplexity': float(np.exp(avg_loss)),\n        'time': time.time() - t0,\n    }\n\n\ndef generate_text(model, prompt, stoi, itos, device, max_len_ctx=128, max_new_tokens=40, temperature=1.0):\n    model.eval()\n    unk_id = stoi['<unk>']\n    eos_id = stoi['<eos>']\n\n    ids = [stoi.get(tok, unk_id) for tok in prompt.split()]\n    if not ids:\n        ids = [eos_id]\n\n    x = torch.tensor([ids], dtype=torch.long, device=device)\n\n    with torch.no_grad():\n        for _ in range(max_new_tokens):\n            if x.size(1) > max_len_ctx:\n                x = x[:, -max_len_ctx:]\n\n            logits = model(x)\n            next_logits = logits[:, -1, :] / max(temperature, 1e-5)\n            probs = torch.softmax(next_logits, dim=-1)\n            next_id = torch.multinomial(probs, num_samples=1)\n            x = torch.cat([x, next_id], dim=1)\n\n            if next_id.item() == eos_id:\n                break\n\n    out_ids = x[0].tolist()\n    out_toks = [itos[i] if 0 <= i < len(itos) else '<unk>' for i in out_ids]\n    return ' '.join(tok for tok in out_toks if tok != '<eos>')\n\n\nprint('Helper functions defined')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_param_count(results, iv_name, output_path):\n    \"\"\"Bar chart of parameter count vs. the independent variable.\"\"\"\n    keys = sorted(results.keys())\n    params = [results[k]['n_params'] for k in keys]\n\n    fig, ax = plt.subplots(figsize=(7, 4))\n    ax.bar([str(k) for k in keys], params, color='steelblue')\n    ax.set_xlabel(iv_name)\n    ax.set_ylabel('Parameters')\n    ax.set_title(f'Parameter Count vs. {iv_name}')\n    ax.grid(True, axis='y', alpha=0.3)\n    for i, (k, p) in enumerate(zip(keys, params)):\n        ax.text(i, p, f'{p:,}', ha='center', va='bottom', fontsize=8)\n    plt.tight_layout()\n    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n    plt.show()\n    print('Saved', output_path)\n\n\ndef plot_loss_curves(results, iv_name, output_path):\n    \"\"\"Train and valid loss curves, one line per setting.\"\"\"\n    keys = sorted(results.keys())\n    colors = plt.cm.viridis(np.linspace(0, 0.9, len(keys)))\n\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    fig.suptitle(f'Loss Curves by {iv_name}', fontsize=14)\n\n    for k, color in zip(keys, colors):\n        data = results[k]\n        epochs = list(range(1, len(data['train_losses']) + 1))\n        label = f'{iv_name}={k} ({data[\"n_params\"]:,} params)'\n        axes[0].plot(epochs, data['train_losses'], color=color, marker='o',\n                     markersize=3, label=label)\n        axes[1].plot(epochs, data['valid_losses'], color=color, marker='o',\n                     markersize=3, label=label)\n\n    axes[0].set_title('Train Loss')\n    axes[1].set_title('Valid Loss')\n    for ax in axes:\n        ax.set_xlabel('Epoch')\n        ax.set_ylabel('Loss')\n        ax.grid(True, alpha=0.3)\n        ax.legend(fontsize=8)\n\n    plt.tight_layout()\n    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n    plt.show()\n    print('Saved', output_path)\n\n\nprint('Plotting functions defined')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results, output_path):\n",
    "    serializable = {}\n",
    "    for depth, data in results.items():\n",
    "        serializable[str(depth)] = {}\n",
    "        for k, v in data.items():\n",
    "            if isinstance(v, (np.floating, np.integer)):\n",
    "                serializable[str(depth)][k] = float(v)\n",
    "            else:\n",
    "                serializable[str(depth)][k] = v\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(serializable, f, indent=2)\n",
    "\n",
    "    print('Saved results to', output_path)\n",
    "\n",
    "\n",
    "print('Result saver defined')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Experiment Runner"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Optional: reduce runtime for quick checks\n# CONFIG['depths'] = [1, 2]\n# CONFIG['widths'] = [64, 128]\n# CONFIG['n_epochs'] = 2\n# CONFIG['max_train_tokens'] = 300000"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_sweep(sweep_values, config, vary='depth'):\n    \"\"\"\n    Train one model per sweep value and collect results.\n\n    Args:\n        sweep_values: list of ints -- n_layers (depth) or d_model (width)\n        config: dict with fixed hyperparameters\n        vary: 'depth' or 'width'\n    \"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f'Device: {device}  |  Sweeping {vary} over {sweep_values}')\n\n    results = {}\n    for val in sweep_values:\n        print('\\n' + '=' * 60)\n        print(f'Training {vary}={val}')\n        print('=' * 60)\n\n        if vary == 'depth':\n            n_layers, d_model, d_ff = val, config['d_model'], config['d_ff']\n        else:  # width\n            n_layers, d_model, d_ff = config['n_layers'], val, val * 4\n\n        model = TransformerDecoder(\n            vocab_size=len(itos),\n            d_model=d_model,\n            n_heads=config['n_heads'],\n            n_layers=n_layers,\n            d_ff=d_ff,\n            dropout=config['dropout'],\n            max_len=config['max_len'],\n        ).to(device)\n\n        n_params = sum(p.numel() for p in model.parameters())\n        print(f'  d_model={d_model}  n_layers={n_layers}  d_ff={d_ff}  params={n_params:,}')\n\n        optimizer = optim.SGD(model.parameters(),\n                              lr=config['lr'],\n                              momentum=config['momentum'],\n                              weight_decay=config['weight_decay'])\n\n        train_losses, valid_losses, valid_ppls = [], [], []\n\n        for epoch in range(1, config['n_epochs'] + 1):\n            t = train_epoch_lm(model, train_loader, optimizer, device,\n                               grad_clip=config['grad_clip'])\n            v = evaluate_lm(model, valid_loader, device)\n            train_losses.append(t['loss'])\n            valid_losses.append(v['loss'])\n            valid_ppls.append(v['perplexity'])\n            print(f\"  Epoch {epoch:2d}/{config['n_epochs']} | \"\n                  f\"train loss {t['loss']:.4f} | \"\n                  f\"valid loss {v['loss']:.4f} | \"\n                  f\"valid ppl {v['perplexity']:.2f} | \"\n                  f\"time {t['time']:.1f}s\")\n\n        test = evaluate_lm(model, test_loader, device)\n        print(f\"  Test loss {test['loss']:.4f} | \"\n              f\"ppl {test['perplexity']:.2f} | \"\n              f\"acc {test['token_acc']:.2%}\")\n\n        results[val] = {\n            'n_params': n_params,\n            'train_losses': train_losses,\n            'valid_losses': valid_losses,\n            'valid_ppls': valid_ppls,\n            'test_loss': test['loss'],\n            'test_perplexity': test['perplexity'],\n            'test_token_acc': test['token_acc'],\n        }\n\n    return results\n\n\nprint('Experiment runner defined')"
  },
  {
   "cell_type": "code",
   "source": "def run_regularization_sweep(sweep_name, sweep_values, build_fn, config,\n                              make_loader_fn=None):\n    \"\"\"\n    Generic sweep runner for regularization / batch-size experiments.\n\n    Args:\n        sweep_name: label for logging/plotting (e.g. \"Dropout\", \"L2\")\n        sweep_values: list of values to iterate over\n        build_fn(val, config): returns (model, optimizer, l1_lambda) for each value\n        config: dict with fixed hyperparameters\n        make_loader_fn(val): optional callback returning a train DataLoader\n                             (used by batch-size sweep)\n    Returns:\n        results dict keyed by sweep value\n    \"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f'Device: {device}  |  Sweeping {sweep_name} over {sweep_values}')\n\n    results = {}\n    for val in sweep_values:\n        print('\\n' + '=' * 60)\n        print(f'Training {sweep_name}={val}')\n        print('=' * 60)\n\n        model, optimizer, l1_lam = build_fn(val, config)\n        model = model.to(device)\n        n_params = sum(p.numel() for p in model.parameters())\n        print(f'  params={n_params:,}  l1_lambda={l1_lam}')\n\n        cur_train_loader = make_loader_fn(val) if make_loader_fn else train_loader\n\n        train_losses, valid_losses, valid_ppls = [], [], []\n\n        for epoch in range(1, config['n_epochs'] + 1):\n            t = train_epoch_lm(model, cur_train_loader, optimizer, device,\n                               grad_clip=config['grad_clip'], l1_lambda=l1_lam)\n            v = evaluate_lm(model, valid_loader, device)\n            train_losses.append(t['loss'])\n            valid_losses.append(v['loss'])\n            valid_ppls.append(v['perplexity'])\n            print(f\"  Epoch {epoch:2d}/{config['n_epochs']} | \"\n                  f\"train loss {t['loss']:.4f} | \"\n                  f\"valid loss {v['loss']:.4f} | \"\n                  f\"valid ppl {v['perplexity']:.2f} | \"\n                  f\"time {t['time']:.1f}s\")\n\n        test = evaluate_lm(model, test_loader, device)\n        print(f\"  Test loss {test['loss']:.4f} | \"\n              f\"ppl {test['perplexity']:.2f} | \"\n              f\"acc {test['token_acc']:.2%}\")\n\n        results[val] = {\n            'n_params': n_params,\n            'train_losses': train_losses,\n            'valid_losses': valid_losses,\n            'valid_ppls': valid_ppls,\n            'test_loss': test['loss'],\n            'test_perplexity': test['perplexity'],\n            'test_token_acc': test['token_acc'],\n        }\n\n    return results\n\n\nprint('Regularization sweep runner defined')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 7. Depth Experiment (vary n_layers, fixed d_model)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "depth_results = run_sweep(CONFIG['depths'], CONFIG, vary='depth')\n\nsave_results(depth_results, os.path.join(OUTPUT_DIR, 'depth_results.json'))\nplot_param_count(depth_results, 'Depth (n_layers)',\n                 os.path.join(OUTPUT_DIR, 'depth_param_count.png'))\nplot_loss_curves(depth_results, 'Depth',\n                 os.path.join(OUTPUT_DIR, 'depth_loss_curves.png'))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Width Experiment (vary d_model, fixed n_layers)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "width_results = run_sweep(CONFIG['widths'], CONFIG, vary='width')\n\nsave_results(width_results, os.path.join(OUTPUT_DIR, 'width_results.json'))\nplot_param_count(width_results, 'Width (d_model)',\n                 os.path.join(OUTPUT_DIR, 'width_param_count.png'))\nplot_loss_curves(width_results, 'Width',\n                 os.path.join(OUTPUT_DIR, 'width_loss_curves.png'))"
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Batch Size Sweep",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def build_batch_size_model(val, config):\n    model = TransformerDecoder(\n        vocab_size=len(itos),\n        d_model=config['reg_d_model'],\n        n_heads=config['n_heads'],\n        n_layers=config['reg_n_layers'],\n        d_ff=config['reg_d_ff'],\n        dropout=config['dropout'],\n        max_len=config['max_len'],\n    )\n    optimizer = optim.SGD(model.parameters(), lr=config['lr'],\n                          momentum=config['momentum'],\n                          weight_decay=config['weight_decay'])\n    return model, optimizer, 0.0\n\ndef make_batch_loader(batch_size):\n    return DataLoader(TensorDataset(train_x, train_y),\n                      batch_size=batch_size, shuffle=True)\n\nbatch_results = run_regularization_sweep(\n    'BatchSize', CONFIG['batch_sizes'], build_batch_size_model, CONFIG,\n    make_loader_fn=make_batch_loader,\n)\n\nsave_results(batch_results, os.path.join(OUTPUT_DIR, 'batch_size_results.json'))\nplot_loss_curves(batch_results, 'BatchSize',\n                 os.path.join(OUTPUT_DIR, 'batch_size_loss_curves.png'))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Dropout Sweep",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def build_dropout_model(val, config):\n    model = TransformerDecoder(\n        vocab_size=len(itos),\n        d_model=config['reg_d_model'],\n        n_heads=config['n_heads'],\n        n_layers=config['reg_n_layers'],\n        d_ff=config['reg_d_ff'],\n        dropout=val,\n        max_len=config['max_len'],\n    )\n    optimizer = optim.SGD(model.parameters(), lr=config['lr'],\n                          momentum=config['momentum'],\n                          weight_decay=config['weight_decay'])\n    return model, optimizer, 0.0\n\ndropout_results = run_regularization_sweep(\n    'Dropout', CONFIG['dropout_rates'], build_dropout_model, CONFIG,\n)\n\nsave_results(dropout_results, os.path.join(OUTPUT_DIR, 'dropout_results.json'))\nplot_loss_curves(dropout_results, 'Dropout',\n                 os.path.join(OUTPUT_DIR, 'dropout_loss_curves.png'))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 11. L2 Regularization Sweep (weight decay)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def build_l2_model(val, config):\n    model = TransformerDecoder(\n        vocab_size=len(itos),\n        d_model=config['reg_d_model'],\n        n_heads=config['n_heads'],\n        n_layers=config['reg_n_layers'],\n        d_ff=config['reg_d_ff'],\n        dropout=config['dropout'],\n        max_len=config['max_len'],\n    )\n    optimizer = optim.SGD(model.parameters(), lr=config['lr'],\n                          momentum=config['momentum'],\n                          weight_decay=val)\n    return model, optimizer, 0.0\n\nl2_results = run_regularization_sweep(\n    'L2 (weight_decay)', CONFIG['l2_weights'], build_l2_model, CONFIG,\n)\n\nsave_results(l2_results, os.path.join(OUTPUT_DIR, 'l2_results.json'))\nplot_loss_curves(l2_results, 'L2 (weight_decay)',\n                 os.path.join(OUTPUT_DIR, 'l2_loss_curves.png'))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. L1 Sweep"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def build_l1_model(val, config):\n    model = TransformerDecoder(\n        vocab_size=len(itos),\n        d_model=config['reg_d_model'],\n        n_heads=config['n_heads'],\n        n_layers=config['reg_n_layers'],\n        d_ff=config['reg_d_ff'],\n        dropout=config['dropout'],\n        max_len=config['max_len'],\n    )\n    optimizer = optim.SGD(model.parameters(), lr=config['lr'],\n                          momentum=config['momentum'],\n                          weight_decay=config['weight_decay'])\n    return model, optimizer, val\n\nl1_results = run_regularization_sweep(\n    'L1', CONFIG['l1_weights'], build_l1_model, CONFIG,\n)\n\nsave_results(l1_results, os.path.join(OUTPUT_DIR, 'l1_results.json'))\nplot_loss_curves(l1_results, 'L1',\n                 os.path.join(OUTPUT_DIR, 'l1_loss_curves.png'))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Elastic Net -- Ratio Sweep\n",
    "\n",
    "Fixed total strength, sweep the L1/L2 ratio (0 = pure L2, 1 = pure L1)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def build_enet_ratio_model(ratio, config):\n    total = config['enet_total_strength']\n    l1_lam = total * ratio\n    wd = total * (1 - ratio)\n    model = TransformerDecoder(\n        vocab_size=len(itos),\n        d_model=config['reg_d_model'],\n        n_heads=config['n_heads'],\n        n_layers=config['reg_n_layers'],\n        d_ff=config['reg_d_ff'],\n        dropout=config['dropout'],\n        max_len=config['max_len'],\n    )\n    optimizer = optim.SGD(model.parameters(), lr=config['lr'],\n                          momentum=config['momentum'],\n                          weight_decay=wd)\n    return model, optimizer, l1_lam\n\nenet_ratio_results = run_regularization_sweep(\n    'ENet Ratio', CONFIG['enet_ratios'], build_enet_ratio_model, CONFIG,\n)\n\nsave_results(enet_ratio_results, os.path.join(OUTPUT_DIR, 'enet_ratio_results.json'))\nplot_loss_curves(enet_ratio_results, 'ENet Ratio',\n                 os.path.join(OUTPUT_DIR, 'enet_ratio_loss_curves.png'))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Elastic Net -- Strength Sweep\n",
    "\n",
    "Fixed L1/L2 ratio, sweep the total regularization strength."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def build_enet_strength_model(strength, config):\n    ratio = config['enet_ratio']\n    l1_lam = strength * ratio\n    wd = strength * (1 - ratio)\n    model = TransformerDecoder(\n        vocab_size=len(itos),\n        d_model=config['reg_d_model'],\n        n_heads=config['n_heads'],\n        n_layers=config['reg_n_layers'],\n        d_ff=config['reg_d_ff'],\n        dropout=config['dropout'],\n        max_len=config['max_len'],\n    )\n    optimizer = optim.SGD(model.parameters(), lr=config['lr'],\n                          momentum=config['momentum'],\n                          weight_decay=wd)\n    return model, optimizer, l1_lam\n\nenet_strength_results = run_regularization_sweep(\n    'ENet Strength', CONFIG['enet_strengths'], build_enet_strength_model, CONFIG,\n)\n\nsave_results(enet_strength_results, os.path.join(OUTPUT_DIR, 'enet_strength_results.json'))\nplot_loss_curves(enet_strength_results, 'ENet Strength',\n                 os.path.join(OUTPUT_DIR, 'enet_strength_loss_curves.png'))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 15. Summary"
   ]
  },
  {
   "cell_type": "code",
   "source": "print('=' * 60)\nprint('ALL EXPERIMENTS COMPLETE')\nprint('=' * 60)\nprint('\\nOutput directory:', OUTPUT_DIR)\nprint('\\nGenerated files:')\nfor f in sorted(os.listdir(OUTPUT_DIR)):\n    path = os.path.join(OUTPUT_DIR, f)\n    if os.path.isfile(path):\n        size_kb = os.path.getsize(path) / 1024\n        print(f'  {f} ({size_kb:.1f} KB)')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "qZWC_MgV0tFV",
    "8nVJH_cK0tFX",
    "p3S_Yf5_0tFZ",
    "2z_yqIVT0tFb"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}